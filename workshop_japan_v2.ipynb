{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS7zWhYK8gn3"
      },
      "source": [
        "# Topological Deep Learning: A new direction for artificial intelligence with healthcare applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‰ΩçÁõ∏Âπæ‰ΩïÂ≠¶„Éª„Éá„Ç£„Éº„Éó„É©„Éº„Éã„É≥„Ç∞: ÂÅ•Â∫∑ÁÆ°ÁêÜ„Å´ÂøúÁî®„Åï„Çå„Çã‰∫∫Â∑•Áü•ËÉΩ„ÅÆÊñ∞„Åü„Å™ÊñπÂêëÊÄß\n",
        "\n",
        "„ÅÑ„Åù„ÅÜ„ÅÑ„Åè„Å™„Çì„Åå„Åè„Éª„Éá„Ç£„Éº„Éó„É©„Éº„Éã„É≥„Ç∞Ôºö„Åë„Çì„Åì„ÅÜ„Å´„Åã„Çì„Çä„Å´„Åä„ÅÜ„Çà„ÅÜ„Åï„Çå„Çã‰∫∫Â∑•Áü•ËÉΩ„ÅÆ„ÅÇ„Çâ„Åü„Å™„Åª„ÅÜ„Åì„ÅÜ„Åõ„ÅÑ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This workshop consists of a notebook. The goals of this notebook are:\n",
        "\n",
        "* Briefly present what are Topological Deep Learning (and Geometric Deep Learning) by providing some explanations, definitions, visuals, etc\n",
        "* Present the potential applications in healthcare and some work that have been done so far in these emerging fields\n",
        "* Run some code to get some hands-on experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Author / Acknowledgment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This workshop has been written by **Adrien Carrel** for the [3rd Big Data Machine Learning in Healthcare in Japan (2023)](https://datathon-japan.jp/2023tokyo/). He holds a MSc degree in Advanced Computing obtained at Imperial College London, United Kingdom, he previously obtained a MEng in Applied Mathematics (Dipl√¥me d'Ing√©nieur) at CentraleSup√©lec, France, and he studied in Classes Pr√©paratoires (MPSI/MP*) in France at Lyc√©e Hoche (Versailles) and Lyc√©e Pierre Corneille (Rouen).\n",
        "\n",
        "Feel free to reach out to me using the links below!\n",
        "\n",
        "Mail: a.carrel@hotmail.fr\n",
        "\n",
        "Tel: +33 7 86 83 27 05\n",
        "\n",
        "<a href=\"https://linkedin.com/in/adrien.carrel/\" target=\"_blank\"><img align=\"center\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/linkedin.svg\" alt=\"linkedin\" height=\"39\" width=\"52\"/></a>\n",
        "<a href=\"https://www.instagram.com/adrien.carrel\" target=\"_blank\"><img align=\"center\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/instagram.svg\" alt=\"instagram\" height=\"39\" width=\"52\" /></a>\n",
        "<a href=\"https://github.com/AdrienC21/\" target=\"_blank\"><img align=\"center\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/github.svg\" alt=\"github\" height=\"39\" width=\"52\" /></a>\n",
        "\n",
        "<img src=\"https://adriencarrel.com/images/avatar.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
        "\n",
        "Special thanks to Tolga Birdal, Mustafa Hajij, Nina Miolane and the rest of the authors in the two papers below. Some ideas have also been inspired by the work of Li et al. (see below) and the Hands-on on graph neural network written by Google. I would also like to thank all the authors of the papers that I mentionned in this notebook.\n",
        "\n",
        "* [Papillon et al. : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)\n",
        "\n",
        "* [Hajij et al. : Topological Deep Learning: Going Beyond Graph Data](https://arxiv.org/abs/2206.00606)\n",
        "\n",
        "* [Li et al. : Graph Representation Learning in Biomedicine and Healthcare (2022)](https://www.nature.com/articles/s41551-022-00942-x)\n",
        "\n",
        "* [Adrien Carrel: Combinatorial Complex Score-based Diffusion Modeling through Stochastic Differential Equations (2023)](https://github.com/AdrienC21/CCSD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topological/Geometric Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What is Machine Learning?**\n",
        "\n",
        "Deep learning is a branch of machine learning that uses neural networks to process data and learn patterns, enabling computers to perform tasks like image recognition, language understanding, and decision-making. It's inspired by the brain's structure, using layers of interconnected nodes (neurons) to extract complex features from input data and make accurate predictions.\n",
        "\n",
        "<img src=\"https://thegradient.pub/content/images/size/w1600/2019/02/1_1mpE6fsq5LNxH31xeTWi5w.jpeg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
        "\n",
        "Source: [The Gradient](https://thegradient.pub/the-limitations-of-visual-deep-learning-and-how-we-might-fix-them/)\n",
        "\n",
        "**What is Topology?**\n",
        "\n",
        "Topology is a branch of mathematics that studies the properties of spaces and shapes, and how similar they are between each other under continuous deformations (shearing, stretching, bending, etc), but not tearing, drilling or gluing! It generalizes these notions without relying on the concept of distance. The goal is to classify and compare different types of spaces and objects by relying on properties such as connectivity, compactness, and continuity.\n",
        "\n",
        "**Question 1:** What is a hole?\n",
        "\n",
        "**Question 2:** How many holes in the human body?\n",
        "\n",
        "<img src=\"https://www.math.ens.psl.eu/~cemprin/formes.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
        "\n",
        "Source: [ENS Travaux dirig√©s (Printemps 2022) : Topologie alg√©brique](https://www.math.ens.psl.eu/~cemprin/enseignementP22.html)\n",
        "\n",
        "<img src=\"https://i.insider.com/57f4f6e8dd08959f358b482f?width=500&format=jpeg&auto=webp\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
        "\n",
        "Source: [Johan Jarnestad/The Royal Swedish Academy of Sciences](https://www.nobelprize.org/nobel_prizes/physics/laureates/2016/popular-physicsprize2016.pdf)\n",
        "\n",
        "**And so ... what are TDL and GDL?**\n",
        "\n",
        "Topological Deep Learning (TDL) and Geometric Deep Learning (GDL) are two new and emerging subfields within the broader fields of machine learning and deep learning that focus on incorporating topological and geometric information into the learning process. Such information is often refered to as **inductive bias**. The idea behind is that, by including such information, the performance of the resulting model will increase.\n",
        "\n",
        "TDL is the combination of (algebraic) topology and machine learning or deep learning. This combination generally consists of transforming the data into topological representations that capture its underlying structure. These modified data are then combined with algorithms, often neural networks, thus improving sometimes their ability to handle complex and high-dimensional data.\n",
        "\n",
        "GDL and TDL sometimes refer to the same things. However, GDL mostly refers to learning from unconventional sources of data with different geometric structures, such as graphs, point clouds, meshes, and manifolds.\n",
        "\n",
        "Overall, the aim of topological deep learning is to create neural network architectures that are invariant or equivariant to certain transformations (as the original objects are). Mathematically, let $f$ be a scalar function, $F$ be a vector function, $X$ be a node feature matrix, $A$ and adjacency matrix, and $\\mathcal{P}$ the set of all the permutations of nodes. The invariance and equivariance are defined by:\n",
        "\n",
        "* $\\forall P\\in\\mathcal{P}, f(PX,PAP^{T})=f(X,A)$\n",
        "\n",
        "* $\\forall P\\in\\mathcal{P}, F(PX,PAP^{T})=PF(X,A)$\n",
        "\n",
        "For reference, here is a nice paper on invariance and equivariance in graph neural networks: [Keriven, N. and Peyr√©, G. Universal invariant and equivariant graph neural networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019](https://papers.nips.cc/paper_files/paper/2019/hash/ea9268cb43f55d1d12380fb6ea5bf572-Abstract.html)\n",
        "\n",
        "Categories of Geometric Deep Learning. Source: [Bronstein, Bruna, Cohen, Velickovic, Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges (2021)](https://arxiv.org/abs/2104.13478).\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J6Ipo8rqdjpsN3_9LafnQw.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "\n",
        "_Important remark:_ TDL/GDL generalize Deep Learning. Here are a few reasons:\n",
        "\n",
        "* Structures we encounter can be represented as grid (e.g. images) or graphs (see [Everything is Connected: Graph Neural Networks\n",
        "](https://arxiv.org/abs/2301.08210)). Therefore, such architectures generalize and could be applied to a wider range of data types (compared to RNN, CNN, etc).\n",
        "\n",
        "* Graphs and other geometric structures also often have sparse connections or missing data.\n",
        "\n",
        "* Connectivity allows interpretability. Also, if such a model performs well, it can inform us about the geometry of the data.\n",
        "\n",
        "* Most of the TDL architectures preserve invariance and equivariance compare to regular neural network architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Neuroscience research:** [Georgiadis K, Kalaganis FP, Oikonomou VP, Nikolopoulos S, Laskaris NA, Kompatsiaris I. RNeuMark: A Riemannian EEG Analysis Framework for Neuromarketing. Brain Inform. (2022)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9481797/)\n",
        "\n",
        "Exploit Riemannian geometry concepts to introduce a novel EEG-based decoder for detecting the consumers' preferences.\n",
        "\n",
        "**Diagnosis of Glaucoma**: [Alexandre H. Thi√©ry, Fabian Braeu, Tin A. Tun, Tin Aung, Micha√´l J. A. Girard; Medical Application of Geometric Deep Learning for the Diagnosis of Glaucoma. Trans. Vis. Sci. Tech. 2023;12(2):23.](https://tvst.arvojournals.org/article.aspx?articleid=2785377)\n",
        "\n",
        "Apply a geometric deep learning solution (PointNet) to provide a robust glaucoma diagnosis from a single OCT (optical coherence tomography) scan of the ONH (optic nerve head).\n",
        "\n",
        "<img src=\"https://arvo.silverchair-cdn.com/arvo/content_public/journal/tvst/938623/m_i2164-2591-12-2-23-f1_1676453203.08458.png?Expires=1695180495&Signature=ubCEKrIBHzw2Lz9vCXZmrrihTgZiSLrgnQUT12Zof6fKV0INJZut6M-GHAtwZ~YHHKWpkzJBwtK9nfnb6oDCoY~SJGye3gtmofXIJRc-4hn1Pt~1BXd7RH4jct~y0WoVtBIN-gI8B7jukfJAu4ZNOzQkn5OA4lwTWcnkj~leDoquDISsuizuS80-0CMIly4YEng~nluOux2cSCxjH5Ys-hM~6dk2AcXseXZY~Ey8iD54jken8cKMxC4-oH9Jbbm~mGMRX3~XShnqCvpTYqagXJ6ASumO8tcOuoHVP-4zzXMsmhY2XYQut19GeEsIkQot8fnYv3toWAhxf5eB9Clj5A__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
        "\n",
        "**Drug virus interactions:** [Das B, Kutsal M, Das R. A geometric deep learning model for display and prediction of potential drug-virus interactions against SARS-CoV-2.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9400382/)\n",
        "\n",
        "Simulate the interactions between molecules and viruses in order to predict if and how drugs will work.\n",
        "\n",
        "<img src=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9400382/bin/gr1_lrg.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
        "\n",
        "**Pathology prediction Chest X-Ray**: [Gaurang A. Karwande. Geometric Deep Learning for Healthcare Applications (2023)](https://vtechworks.lib.vt.edu/bitstream/handle/10919/115361/Karwande_GA_T_2023.pdf?sequence=1&isAllowed=y)\n",
        "\n",
        "Construct a graphical representation of the CXR (Chest X-Ray) image pairs by utilizing the correlation\n",
        "among anatomical region features from the images, and the correlation among anatomical regions between the two images in the pair.\n",
        "\n",
        "**Drug prediction, discovery, and molecular conformer generation**:\n",
        "\n",
        "[Shen, C., Luo, J., & Xia, K. (2023). Molecular geometric deep learning.](https://arxiv.org/abs/2306.15065)\n",
        "\n",
        "[Adrien Carrel. Combinatorial Complex Score-based Diffusion Modeling through Stochastic Differential Equations. (2023)](https://github.com/AdrienC21/CCSD)\n",
        "\n",
        "[Jing, B., Corso, G., Chang, J., Barzilay, R., & Jaakkola, T. (2022). Torsional Diffusion for Molecular Conformer Generation.](https://arxiv.org/abs/2206.01729)\n",
        "\n",
        "Model the drugs or molecules as graphs other topological structures and predict their intrinsic properties like their solubility in a particular fluid. Generate some new molecules that follow a particular distribution.\n",
        "\n",
        "<img src=\"https://adriencarrel.com/images/torsional.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminary steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installation of sone packages and the QM9 dataset! (qm9.csv to download on your machine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kdCOurgo-vh7",
        "outputId": "34792e50-19b4-4053-9703-4d7df62e8e03"
      },
      "outputs": [],
      "source": [
        "# Let's install PyTorch and TopoModelX\n",
        "# For GPU, change \"cpu\" to \"cu118\" or your cuda version\n",
        "!pip install torch==2.0.1 --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cpu.html\n",
        "!pip install git+https://github.com/pyt-team/TopoModelX@ff0425a825311f9f80b6ca784bae128b11ccb827\n",
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RESTART YOUR KERNEL/RUNTIME** after the installation(s)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "General imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DWDUTHh8gn5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "from typing import Optional, Tuple, List, Union, Dict, Any, Callable\n",
        "from time import perf_counter\n",
        "\n",
        "import torch\n",
        "import torch_geometric\n",
        "import random\n",
        "import hypernetx\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "from torch.nn import Linear\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import KarateClub\n",
        "from torch_geometric.utils import to_networkx\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tgFv-Ci8gn6"
      },
      "source": [
        "Load GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfxz2Xxd8gn6",
        "outputId": "f754cf1f-1560-49c3-cd92-66af3addb1f2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set seed for interpretability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mWb00mQChVa"
      },
      "source": [
        "## Introduction: Graph Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preamble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graph neural networks (GNNs) are a type of geometric deep learning models to process ... surprise ... graphs! It consists of iteratively updating the features of each node (or edges) in the graph, taking into account the features of its neighbors = the geometry of our data.\n",
        "\n",
        "**Definition of a (Undirected) Graph:**\n",
        "\n",
        "Let $S$ be a non-empty set. A graph on $S$ is a pair $\\mathcal{G}=(S, \\mathcal{E})$ where $\\mathcal{E}$ is a set of non-empty subsets of **size 2** of the powerset $\\mathcal{P}(S)$ of $S$, which are called edges. Elements of $S$ are called vertices.\n",
        "\n",
        "A graph is said to be directed if for all $(u, v)\\in \\mathcal{E}$, $(v, u)\\in \\mathcal{E}$.\n",
        "\n",
        "------\n",
        "\n",
        "<img src=\"https://adriencarrel.com/images/GNN.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
        "\n",
        "In this part, we will introduce you to the basics of GNNs using the [PyTorch](https://pytorch.org/) and [PyTorch Geometric (PyG)](https://github.com/rusty1s/pytorch_geometric) libraries. PyG is an extension of PyTorch that provides a variety of tools for working with graph data. We will start by working on the **Zachary's Karate club dataset**. It consists of 34 nodes, which represent members of a university Karate club, and 78 edges, which represent connections between members who interacted outside of the club. During the study, a conflict arose between the administrator \"John A\" and instructor \"Mr. Hi\" (there are pseudonyms), which led to the split of the club into two. Half of the members formed a new club around Mr. Hi, members from the other part found a new instructor or gave up karate. They also provided labels for 4 classes classification.\n",
        "\n",
        "Source: [\"An Information Flow Model for Conflict and Fission in Small Groups\" by Wayne W. Zachary.](https://en.wikipedia.org/wiki/Zachary%27s_karate_club)\n",
        "\n",
        "We will then train a GNN to detect communities in the Karate club network and predict in which group members will go. The GNN will learn to predict the community label for each node in the network. The model will be trained on a subset of nodes and will be evaluated on a test set which consists on the rest of the nodes. The accuracy will be our measure for the performance of the model.\n",
        "\n",
        "Mathematically,\n",
        "\n",
        "This is done by following a simple **neural message passing scheme**, where node features $\\mathbf{x}_v^{(\\ell)}$ of all nodes $v \\in S$ at layer $l\\in\\mathbb{N}$ in a graph $\\mathcal{G} = (S, \\mathcal{E})$ are iteratively updated by aggregating localized information from their neighbors $\\mathcal{N}(v)$:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = f^{(\\ell + 1)}_{\\theta} \\left( \\mathbf{x}_v^{(\\ell)}, \\left\\{ \\mathbf{x}_w^{(\\ell)} : w \\in \\mathcal{N}(v) \\right\\} \\right)\n",
        "$$\n",
        "\n",
        "where $\\mathcal{N}(v)=\\{ w\\in S, (w, v)\\in\\mathcal{E} \\}$\n",
        "\n",
        "---------\n",
        "\n",
        "The graph neural network that will be implemented below will be made of multiple **Graph Convolutional Network layers (GCN)**: [Kipf, T. N., & Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907). This layer is defined by the following operator:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W}^{(\\ell + 1)}$ are learnable parameters represented as a matrix of size `[nb_output_features, nb_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge $(w,v)$.\n",
        "\n",
        "This layer is already implemented in PyTorch Geometric: [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv). It takes as an input the node feature representation `x` and the graph connectivity representation `edge_index` created by PyTorch Geometric when we loaded our data. The tensor `edge_index` is represented as a **COO** format (coordinate format) commonly used for representing sparse matrices. The idea is that, instead of storing the adjacency information in a dense representation $A \\in \\{ 0, 1 \\}^{|S| \\times |S|}$, PyTorch Geometric represents graphs sparsely, which refers to only holding the coordinates/values for which entries in $A$ are non-zero. This redices space and time complexity.\n",
        "\n",
        "In particular, for this implementation, the weights and normalization coefficients are designed such that:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} =\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}} \\mathbf{x}_v^{(\\ell)} \\mathbf{W}^{(\\ell + 1)}\n",
        "$$\n",
        "\n",
        "where $\\hat{A}=A+I$ is the adjacency matrix with self-loops added and $\\hat{D}$ is a diagonal degree matrix defined by: $\\forall u\\in S, \\hat{D}_{u,u}=\\sum_{v\\in S}\\hat{A}_{u,v}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preliminary analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OpoAhfyCeaZ"
      },
      "outputs": [],
      "source": [
        "dataset = KarateClub()\n",
        "print(f\"Dataset: {dataset}:\")\n",
        "print(\"======================\")\n",
        "print(f\"Number of graphs: {len(dataset)}\")\n",
        "print(f\"Number of features: {dataset.num_features}\")\n",
        "print(f\"Number of classes: {dataset.num_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print information about our graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0vCCKL3CkI6"
      },
      "outputs": [],
      "source": [
        "data = dataset[0]  # Get the graph object.\n",
        "\n",
        "print(data)\n",
        "print(f\"Type: {type(data)}\")\n",
        "print(\"==============================================================\")\n",
        "\n",
        "# Add test_mask: We train on 1 node per class only!\n",
        "data.test_mask = ~data.train_mask\n",
        "print(f\"Test set size: {round(100 * data.test_mask.sum().item() / data.num_nodes, 3)}%\")\n",
        "\n",
        "# Gather some statistics about the dataset/graph.\n",
        "print(f\"Number of nodes: {data.num_nodes}\")\n",
        "print(f\"Number of edges: {data.num_edges}\")\n",
        "# TODO: avg_node_degree  # calculate the average number of edges per node\n",
        "print(f\"Average node degree: {avg_node_degree}\")\n",
        "print(f\"Number of training nodes: {data.train_mask.sum()}\")\n",
        "print(f\"Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}\")\n",
        "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
        "print(f\"Has self-loops: {data.has_self_loops()}\")\n",
        "is_undirected = False  # TODO: to modify, is_undirected should be eqal to True if the graph is undirected\n",
        "print(f\"Is undirected: {is_undirected}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGeNGs50CsOA"
      },
      "outputs": [],
      "source": [
        "def visualize_graph(G: nx.Graph, color: torch.Tensor) -> None:\n",
        "    \"\"\"Visualize a networkx graph.\n",
        "\n",
        "    Args:\n",
        "        G (nx.Graph): A networkx graph.\n",
        "        color (torch.Tensor): A color for the nodes.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    # TODO: complete the code to visualize the graph with the nodes colored according to the color tensor\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_embedding(\n",
        "    h: torch.Tensor,\n",
        "    color: torch.Tensor,\n",
        "    epoch: Optional[int] = None,\n",
        "    loss: Optional[torch.Tensor] = None,\n",
        ") -> None:\n",
        "    \"\"\"Visualize a 2D embedding.\n",
        "\n",
        "    Args:\n",
        "        h (torch.Tensor): A 2D embedding.\n",
        "        color (torch.Tensor): Colors for the nodes.\n",
        "        epoch (Optional[int], optional): The current epoch. Defaults to None.\n",
        "        loss (Optional[torch.Tensor], optional): The current loss. Defaults to None.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    h = h.detach().cpu().numpy()\n",
        "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "    if epoch is not None and loss is not None:\n",
        "        plt.title(f\"Epoch: {epoch}, Loss: {loss.item():.4f}\", fontsize=16)\n",
        "    plt.xlabel(\"h0\")\n",
        "    plt.ylabel(\"h1\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the Karate Club graph dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCXmaGUZCp9g"
      },
      "outputs": [],
      "source": [
        "G = to_networkx(data, to_undirected=True)\n",
        "# TODO: visualize the graph using your function and the label accessible using \"data.y\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3zMQa-vCuby"
      },
      "source": [
        "### Implementing/training Graph Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's implement and train our neural network!\n",
        "\n",
        "We create the network architecture by creating a class that inherits `torch.nn.Module`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdqV2jIFCvcf"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"Our simple GCN model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_features: int,\n",
        "        hidden_dim: int = 4,\n",
        "        embedding_dim: int = 2,\n",
        "        num_classes: int = 4,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the model.\n",
        "\n",
        "        Args:\n",
        "            num_features (int): The number of input features.\n",
        "            hidden_dim (int, optional): The hidden dimension. Defaults to 4.\n",
        "            embedding_dim (int, optional): The embedding dimension. Best to set it to 2 for embeddings interpretability. Defaults to 2.\n",
        "            num_classes (int, optional): The number of classes. Defaults to 4.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.conv1 = GCNConv(\n",
        "            self.num_features, self.hidden_dim\n",
        "        )  # 32, hidden_dim for Karate\n",
        "        self.conv2 = GCNConv(self.hidden_dim, self.hidden_dim)\n",
        "        self.conv3 = GCNConv(self.hidden_dim, self.embedding_dim)\n",
        "        self.classifier = Linear(2, self.num_classes)  # embedding_dim, 4 for Karate\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        \"\"\"Reset the parameters.\"\"\"\n",
        "        self.conv1.reset_parameters()\n",
        "        self.conv2.reset_parameters()\n",
        "        self.conv3.reset_parameters()\n",
        "        self.classifier.reset_parameters()\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, edge_index: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input node features.\n",
        "            edge_index (torch.Tensor): The graph connectivity.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: The output logits for each node classification, and the embedding h.\n",
        "        \"\"\"\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = h.tanh()\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = h.tanh()\n",
        "        h = self.conv3(h, edge_index)\n",
        "        h = h.tanh()\n",
        "        # Last layer h is the embedding\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "\n",
        "        # TODO: complete the code to apply the final classifier on the embedding h. The result should be stored in the variable \"out\".\n",
        "\n",
        "        return out, h\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Representation of the model.\"\"\"\n",
        "        return f\"{self.__class__.__name__}(num_features={self.num_features}, hidden_dim={self.hidden_dim}, embedding_dim={self.embedding_dim}, num_classes={self.num_classes})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GCN(\n",
        "    num_features=dataset.num_features,\n",
        "    hidden_dim=4,\n",
        "    embedding_dim=2,\n",
        "    num_classes=dataset.num_classes,\n",
        ").to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the number of parameters of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_nb_parameters(model: torch.nn.Module) -> int:\n",
        "    \"\"\"Get the number of trainable parameters of a model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of trainable parameters.\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"Number of parameters GCN: {get_nb_parameters(model)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot our initial embeddings with the model not trained\n",
        "\n",
        "The idea is that we can visualize the quality of the training of our model through the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD_taP8pCzMw"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    _, h = model(data.x, data.edge_index)\n",
        "\n",
        "print(f\"Embedding shape: {list(h.shape)}\")\n",
        "\n",
        "visualize_embedding(h, color=data.y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without any training, it is random :/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a loss and an optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.01  # TODO: try a different learning rate?\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()  # cross entropy loss for classification.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's train our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model: torch.nn.Module, data: torch_geometric.data.data.Data\n",
        ") -> Tuple[float, float, float, float, torch.Tensor]:\n",
        "    \"\"\"Train the model for 1 epoch.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        data (torch_geometric.data.data.Data): The graph data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float, float, float, torch.Tensor]: The losses on train and test, the accuracy on train and test, and the embedding h after one epoch/step.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # clear gradients.\n",
        "    out, h = model(data.x, data.edge_index)  # perform a single forward pass.\n",
        "    loss_train = criterion(\n",
        "        out[data.train_mask], data.y[data.train_mask]\n",
        "    )  # compute the loss on the training nodes.\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # no gradients for test pass.\n",
        "        loss_test = criterion(out[data.test_mask], data.y[data.test_mask])\n",
        "        acc_train = (\n",
        "            torch.sum(\n",
        "                out[data.train_mask].argmax(dim=1) == data.y[data.train_mask]\n",
        "            ).item()\n",
        "            / data.train_mask.sum().item()\n",
        "        )\n",
        "        acc_test = (\n",
        "            torch.sum(\n",
        "                out[data.test_mask].argmax(dim=1) == data.y[data.test_mask]\n",
        "            ).item()\n",
        "            / data.test_mask.sum().item()\n",
        "        )\n",
        "    model.train()\n",
        "    loss_train.backward()  # derive gradients.\n",
        "    optimizer.step()  # update parameters based on gradients.\n",
        "    return loss_train.item(), loss_test.item(), acc_train, acc_test, h\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: torch.nn.Module,\n",
        "    data: Union[\n",
        "        torch_geometric.data.data.Data,\n",
        "        Tuple[\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "            torch.Tensor,\n",
        "        ],\n",
        "    ],\n",
        "    nb_epochs: int,\n",
        "    train_func: Callable[\n",
        "        [\n",
        "            torch.nn.Module,\n",
        "            Union[\n",
        "                torch_geometric.data.data.Data,\n",
        "                Tuple[\n",
        "                    torch.Tensor,\n",
        "                    torch.Tensor,\n",
        "                    torch.Tensor,\n",
        "                    torch.Tensor,\n",
        "                    torch.Tensor,\n",
        "                    torch.Tensor,\n",
        "                ],\n",
        "            ],\n",
        "        ],\n",
        "        Tuple[float, float, float, float, torch.Tensor],\n",
        "    ],\n",
        ") -> Tuple[List[float], List[float], List[float], List[float], List[torch.Tensor]]:\n",
        "    \"\"\"Train the model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        data (Union[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]): The graph data.\n",
        "        nb_epochs (int): The number of epochs.\n",
        "        train_func (Callable[[torch.nn.Module, Union[torch_geometric.data.data.Data, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]], Tuple[float, float, float, float torch.Tensor]]): function that train our model for one epoch\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[float], List[float], List[float], List[float], List[torch.Tensor]]: The losses on train and test, the accuracy on train and test, and the embeddings at each epochs.\n",
        "    \"\"\"\n",
        "    top = perf_counter()  # calculate total training time\n",
        "    model.train()\n",
        "    losses_train = []\n",
        "    losses_test = []\n",
        "    accuracy_train = []\n",
        "    accuracy_test = []\n",
        "    embeddings = []\n",
        "    for epoch in tqdm(range(1, nb_epochs + 1)):\n",
        "        loss_train, loss_test, acc_train, acc_test, h = train_func(model, data)\n",
        "        losses_train.append(loss_train)\n",
        "        losses_test.append(loss_test)\n",
        "        accuracy_train.append(acc_train)\n",
        "        accuracy_test.append(acc_test)\n",
        "        embeddings.append(h)\n",
        "    print(f\"Training finished! Total time: {round(perf_counter() - top, 3)}s.\")\n",
        "    return losses_train, losses_test, accuracy_train, accuracy_test, embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model for 400 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_epochs = 400  # TODO: try a different number of epochs?\n",
        "losses_train, losses_test, accuracy_train, accuracy_test, embeddings = train(\n",
        "    model, data, nb_epochs, train_one_epoch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the metrics over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_learning_curves(\n",
        "    losses_train: List[float],\n",
        "    losses_test: List[float],\n",
        "    accuracy_train: List[float],\n",
        "    accuracy_test: List[float],\n",
        ") -> None:\n",
        "    \"\"\"Plot the learning curves.\n",
        "\n",
        "    Args:\n",
        "        losses_train (List[float]): The training losses.\n",
        "        losses_test (List[float]): The test losses.\n",
        "        accuracy_train (List[float]): The training accuracies.\n",
        "        accuracy_test (List[float]): The test accuracies.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(losses_train, label=\"train\")\n",
        "    plt.plot(losses_test, label=\"test\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    # TODO: modify the code to plot the accuracy but in percentage!\n",
        "    plt.plot(np.array(accuracy_train), label=\"train\")\n",
        "    plt.plot(np.array(accuracy_test), label=\"test\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "    # TODO: modify the code to display the accuracies in percentage too!\n",
        "    plt.suptitle(\n",
        "        \"Learning curves. Final train accuracy: {:.2f}%. Final test accuracy: {:.2f}%\".format(\n",
        "            accuracy_train[-1], accuracy_test[-1]\n",
        "        )\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot the learning curves using the function above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot some of the embeddings during the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_multiple_embeddings(\n",
        "    embeddings: List[torch.Tensor],\n",
        "    losses_train: List[float],\n",
        "    losses_test: List[float],\n",
        "    epochs: List[int],\n",
        ") -> None:\n",
        "    \"\"\"Plot the embeddings at each epoch of the list.\n",
        "\n",
        "    Args:\n",
        "        embeddings (List[torch.Tensor]): The embeddings at each epoch.\n",
        "        losses_train (List[float]): The training losses.\n",
        "        losses_test (List[float]): The test losses.\n",
        "        epochs (List[int]): The epochs (starting from 1!).\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    for epoch in epochs:\n",
        "        h = embeddings[epoch - 1]\n",
        "        plt.scatter(\n",
        "            h[:, 0].detach().numpy(),\n",
        "            h[:, 1].detach().numpy(),\n",
        "            c=data.y,\n",
        "            s=140,\n",
        "            cmap=\"Set2\",\n",
        "        )\n",
        "        plt.title(\n",
        "            f\"Epoch: {epoch}, loss_train: {losses_train[epoch-1]:.4f}, loss_test: {losses_test[epoch-1]:.4f}\"\n",
        "        )\n",
        "        plt.xlabel(\"h0\")\n",
        "        plt.ylabel(\"h1\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = [1, 10, 50, 100, 200, 400]  # TODO: try different epochs?\n",
        "plot_multiple_embeddings(embeddings, losses_train, losses_test, epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For binary classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = np.array(\n",
        "    [\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        0,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        0,\n",
        "        0,\n",
        "        1,\n",
        "        1,\n",
        "        0,\n",
        "        1,\n",
        "        0,\n",
        "        1,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "    ]\n",
        ")\n",
        "\n",
        "data = dataset[0]\n",
        "data.y = torch.tensor(y, dtype=torch.long)\n",
        "test_size = 0.2  # TODO: try a different test size?\n",
        "y_train, y_test, ind_train, ind_test = train_test_split(\n",
        "    y, np.arange(data.num_nodes), test_size=test_size, random_state=seed, stratify=y\n",
        ")\n",
        "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.train_mask[ind_train] = True\n",
        "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.test_mask[ind_test] = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_epochs = 400  # TODO: try a different number of epochs?\n",
        "losses_train, losses_test, accuracy_train, accuracy_test, embeddings = train(\n",
        "    model, data, nb_epochs, train_one_epoch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot the learning curves using the function above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second example: Train a Hypergraph Networks with Hyperedge Neurons (HNHN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are some other topological domains that we could use to try to improve our performance:\n",
        "\n",
        "<img src=\"https://adriencarrel.com/images/topological_domains.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For now, let's dive into a more complex Topological Deep Learning framework. We will **lift** our dataset into a hypergraph and we will train on it a Hypergraph Networks with Hyperedge Neurons in the hypergraph domain, as introduced in the paper: [Dong et al. : HNHN: Hypergraph networks with hyperedge neurons (2020)](https://grlplus.github.io/papers/40.pdf). We will also apply it on the Karate Club dataset.\n",
        "\n",
        "But first, what are hypergraphs?\n",
        "\n",
        "**Hypergraph:** Let $S$ be a non-empty set. A hypergraph on $S$ is a pair $(S, X)$, where $X$ is a set of non-empty subsets of the powerset $\\mathcal{P}(S)$ of $S$, which are called hyperedges. Elements of $S$ are called vertices.\n",
        "\n",
        "-------\n",
        "\n",
        "The equations of one layer of the HNHN neural network are given by:\n",
        "\n",
        "Message passing:\n",
        "\n",
        "üü• $\\quad m_{y \\rightarrow x}^{(0 \\rightarrow 1)} = \\sigma((B_1^T \\cdot W^{(0)})_{xy} \\cdot h_y^{t,(0)} \\cdot \\Theta^{t,(0)} + b^{t,(0)})$\n",
        "\n",
        "üü• $\\quad m_{y \\rightarrow x}^{(1 \\rightarrow 0)}  = \\sigma((B_1 \\cdot W^{(1)})_{xy} \\cdot h_y^{t,(1)} \\cdot \\Theta^{t,(1)} + b^{t,(1)})$\n",
        "\n",
        "Within-Neighborhood Aggregation:\n",
        "\n",
        "üüß $\\quad m_x^{(0 \\rightarrow 1)}  = \\sum_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(0 \\rightarrow 1)}$\n",
        "\n",
        "üüß $\\quad m_x^{(1 \\rightarrow 0)}  = \\sum_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(1 \\rightarrow 0)}$\n",
        "\n",
        "Between-Neighborhood Aggregation:\n",
        "\n",
        "üü© $\\quad m_x^{(0)}  = m_x^{(1 \\rightarrow 0)}$\n",
        "\n",
        "üü© $\\quad m_x^{(1)}  = m_x^{(0 \\rightarrow 1)}$\n",
        "\n",
        "Update:\n",
        "\n",
        "üü¶ $\\quad h_x^{t+1,(0)} = m_x^{(0)}$\n",
        "\n",
        "üü¶ $\\quad h_x^{t+1,(1)} = m_x^{(1)}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the models and utility functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Scatter adaptated from torch_scatter/scatter.py from: [torch_scatter](https://github.com/rusty1s/pytorch_scatter/blob/master/torch_scatter/scatter.py)\n",
        "* Message passing and HNHN architecture from TopoModelX: [Message passing](https://github.com/pyt-team/TopoModelX/blob/main/topomodelx/base/message_passing.py) and [HNHN]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def broadcast(src: torch.Tensor, other: torch.Tensor, dim: int):\n",
        "    \"\"\"Broadcasts `src` to the shape of `other`.\"\"\"\n",
        "    if dim < 0:\n",
        "        dim = other.dim() + dim\n",
        "    if src.dim() == 1:\n",
        "        for _ in range(0, dim):\n",
        "            src = src.unsqueeze(0)\n",
        "    for _ in range(src.dim(), other.dim()):\n",
        "        src = src.unsqueeze(-1)\n",
        "    src = src.expand(other.size())\n",
        "    return src\n",
        "\n",
        "\n",
        "def scatter_sum(\n",
        "    src: torch.Tensor,\n",
        "    index: torch.Tensor,\n",
        "    dim: int = -1,\n",
        "    out: Optional[torch.Tensor] = None,\n",
        "    dim_size: Optional[int] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Add all values from the `src` tensor into `out` at the indices.\"\"\"\n",
        "    index = broadcast(index, src, dim)\n",
        "    if out is None:\n",
        "        size = list(src.size())\n",
        "        if dim_size is not None:\n",
        "            size[dim] = dim_size\n",
        "        elif index.numel() == 0:\n",
        "            size[dim] = 0\n",
        "        else:\n",
        "            size[dim] = int(index.max()) + 1\n",
        "        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n",
        "        return out.scatter_add_(dim, index, src)\n",
        "    else:\n",
        "        return out.scatter_add_(dim, index, src)\n",
        "\n",
        "\n",
        "def scatter_add(\n",
        "    src: torch.Tensor,\n",
        "    index: torch.Tensor,\n",
        "    dim: int = -1,\n",
        "    out: Optional[torch.Tensor] = None,\n",
        "    dim_size: Optional[int] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Add all values from the `src` tensor into `out` at the indices.\"\"\"\n",
        "    return scatter_sum(src, index, dim, out, dim_size)\n",
        "\n",
        "\n",
        "def scatter_mean(\n",
        "    src: torch.Tensor,\n",
        "    index: torch.Tensor,\n",
        "    dim: int = -1,\n",
        "    out: Optional[torch.Tensor] = None,\n",
        "    dim_size: Optional[int] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Compute the mean value of all values from the `src` tensor into `out`.\"\"\"\n",
        "    out = scatter_sum(src, index, dim, out, dim_size)\n",
        "    dim_size = out.size(dim)\n",
        "\n",
        "    index_dim = dim\n",
        "    if index_dim < 0:\n",
        "        index_dim = index_dim + src.dim()\n",
        "    if index.dim() <= index_dim:\n",
        "        index_dim = index.dim() - 1\n",
        "\n",
        "    ones = torch.ones(index.size(), dtype=src.dtype, device=src.device)\n",
        "    count = scatter_sum(ones, index, index_dim, None, dim_size)\n",
        "    count[count < 1] = 1\n",
        "    count = broadcast(count, out, dim)\n",
        "    if out.is_floating_point():\n",
        "        out.true_divide_(count)\n",
        "    else:\n",
        "        out.div_(count, rounding_mode=\"floor\")\n",
        "    return out\n",
        "\n",
        "\n",
        "SCATTER_DICT = {\"sum\": scatter_sum, \"mean\": scatter_mean, \"add\": scatter_sum}\n",
        "\n",
        "\n",
        "def scatter(scatter):\n",
        "    \"\"\"Return the scatter function.\"\"\"\n",
        "    if isinstance(scatter, str) and scatter in SCATTER_DICT:\n",
        "        return SCATTER_DICT[scatter]\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"scatter must be callable or string: {list(SCATTER_DICT.keys())}\"\n",
        "        )\n",
        "\n",
        "\n",
        "class MessagePassing(torch.nn.Module):\n",
        "    \"\"\"MessagePassing.\n",
        "\n",
        "    This class defines message passing through a single neighborhood N,\n",
        "    by decomposing it into 2 steps:\n",
        "\n",
        "    1. üü• Create messages going from source cells to target cells through N.\n",
        "    2. üüß Aggregate messages coming from different sources cells onto each target cell.\n",
        "\n",
        "    This class should not be instantiated directly, but rather inherited\n",
        "    through subclasses that effectively define a message passing function.\n",
        "\n",
        "    This class does not have trainable weights, but its subclasses should\n",
        "    define these weights.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    aggr_func : string\n",
        "        Aggregation function to use.\n",
        "    att : bool\n",
        "        Whether to use attention.\n",
        "    initialization : string\n",
        "        Initialization method for the weights of the layer.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [H23] Hajij, Zamzmi, Papamarkou, Miolane, Guzm√°n-S√°enz, Ramamurthy, Birdal, Dey,\n",
        "        Mukherjee, Samaga, Livesay, Walters, Rosen, Schaub. Topological Deep Learning: Going Beyond Graph Data.\n",
        "        (2023) https://arxiv.org/abs/2206.00606.\n",
        "\n",
        "    .. [PSHM23] Papillon, Sanborn, Hajij, Miolane.\n",
        "        Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.\n",
        "        (2023) https://arxiv.org/abs/2304.10031.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        aggr_func=\"sum\",\n",
        "        att=False,\n",
        "        initialization=\"xavier_uniform\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.aggr_func = aggr_func\n",
        "        self.att = att\n",
        "        self.initialization = initialization\n",
        "        assert initialization in [\"xavier_uniform\", \"xavier_normal\"]\n",
        "        assert aggr_func in [\"sum\", \"mean\", \"add\"]\n",
        "\n",
        "    def reset_parameters(self, gain=1.414):\n",
        "        r\"\"\"Reset learnable parameters.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        This function will be called by subclasses of\n",
        "        MessagePassing that have trainable weights.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        gain : float\n",
        "            Gain for the weight initialization.\n",
        "        \"\"\"\n",
        "        if self.initialization == \"xavier_uniform\":\n",
        "            if self.weight is not None:\n",
        "                torch.nn.init.xavier_uniform_(self.weight, gain=gain)\n",
        "            if self.att:\n",
        "                torch.nn.init.xavier_uniform_(self.att_weight.view(-1, 1), gain=gain)\n",
        "\n",
        "        elif self.initialization == \"xavier_normal\":\n",
        "            if self.weight is not None:\n",
        "                torch.nn.init.xavier_normal_(self.weight, gain=gain)\n",
        "            if self.att:\n",
        "                torch.nn.init.xavier_normal_(self.att_weight.view(-1, 1), gain=gain)\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                \"Initialization method not recognized. \"\n",
        "                \"Should be either xavier_uniform or xavier_normal.\"\n",
        "            )\n",
        "\n",
        "    def message(self, x_source, x_target=None):\n",
        "        \"\"\"Construct message from source cells to target cells.\n",
        "\n",
        "        üü• This provides a default message function to the message passing scheme.\n",
        "\n",
        "        Alternatively, users can subclass MessagePassing and overwrite\n",
        "        the message method in order to replace it with their own message mechanism.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_source : Tensor, shape=[..., n_source_cells, in_channels]\n",
        "            Input features on source cells.\n",
        "            Assumes that all source cells have the same rank r.\n",
        "        x_target : Tensor, shape=[..., n_target_cells, in_channels]\n",
        "            Input features on target cells.\n",
        "            Assumes that all target cells have the same rank s.\n",
        "            Optional. If not provided, x_target is assumed to be x_source,\n",
        "            i.e. source cells send messages to themselves.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        _ : Tensor, shape=[..., n_source_cells, in_channels]\n",
        "            Messages on source cells.\n",
        "        \"\"\"\n",
        "        return x_source\n",
        "\n",
        "    def attention(self, x_source, x_target=None):\n",
        "        \"\"\"Compute attention weights for messages.\n",
        "\n",
        "        This provides a default attention function to the message passing scheme.\n",
        "\n",
        "        Alternatively, users can subclass MessagePassing and overwrite\n",
        "        the attention method in order to replace it with their own attention mechanism.\n",
        "\n",
        "        Details in [H23]_, Definition of \"Attention Higher-Order Message Passing\".\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_source : torch.Tensor, shape=[n_source_cells, in_channels]\n",
        "            Input features on source cells.\n",
        "            Assumes that all source cells have the same rank r.\n",
        "        x_target : torch.Tensor, shape=[n_target_cells, in_channels]\n",
        "            Input features on source cells.\n",
        "            Assumes that all source cells have the same rank r.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        _ : torch.Tensor, shape = [n_messages, 1]\n",
        "            Attention weights: one scalar per message between a source and a target cell.\n",
        "        \"\"\"\n",
        "        x_source_per_message = x_source[self.source_index_j]\n",
        "        x_target_per_message = (\n",
        "            x_source[self.target_index_i]\n",
        "            if x_target is None\n",
        "            else x_target[self.target_index_i]\n",
        "        )\n",
        "\n",
        "        x_source_target_per_message = torch.cat(\n",
        "            [x_source_per_message, x_target_per_message], dim=1\n",
        "        )\n",
        "\n",
        "        return torch.nn.functional.elu(\n",
        "            torch.matmul(x_source_target_per_message, self.att_weight)\n",
        "        )\n",
        "\n",
        "    def aggregate(self, x_message):\n",
        "        \"\"\"Aggregate messages on each target cell.\n",
        "\n",
        "        A target cell receives messages from several source cells.\n",
        "        This function aggregates these messages into a single output\n",
        "        feature per target cell.\n",
        "\n",
        "        üüß This function corresponds to the within-neighborhood aggregation\n",
        "        defined in [H23]_ and [PSHM23]_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_messages : Tensor, shape=[..., n_messages, out_channels]\n",
        "            Features associated with each message.\n",
        "            One message is sent from a source cell to a target cell.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        _ : Tensor, shape=[...,  n_target_cells, out_channels]\n",
        "            Output features on target cells.\n",
        "            Each target cell aggregates messages from several source cells.\n",
        "            Assumes that all target cells have the same rank s.\n",
        "        \"\"\"\n",
        "        aggr = scatter(self.aggr_func)\n",
        "        return aggr(x_message, self.target_index_i, 0)\n",
        "\n",
        "    def forward(self, x_source, neighborhood, x_target=None):\n",
        "        r\"\"\"Forward pass.\n",
        "\n",
        "        This implements message passing for a given neighborhood:\n",
        "\n",
        "        - from source cells with input features `x_source`,\n",
        "        - via `neighborhood` defining where messages can pass,\n",
        "        - to target cells with input features `x_target`.\n",
        "\n",
        "        In practice, this will update the features on the target cells.\n",
        "\n",
        "        If not provided, x_target is assumed to be x_source,\n",
        "        i.e. source cells send messages to themselves.\n",
        "\n",
        "        The message passing is decomposed into two steps:\n",
        "\n",
        "        1. üü• Message: A message :math:`m_{y \\rightarrow x}^{\\left(r \\rightarrow s\\right)}`\n",
        "        travels from a source cell :math:`y` of rank r to a target cell :math:`x` of rank s\n",
        "        through a neighborhood of :math:`x`, denoted :math:`\\mathcal{N} (x)`,\n",
        "        via the message function :math:`M_\\mathcal{N}`:\n",
        "\n",
        "        .. math::\n",
        "            m_{y \\rightarrow x}^{\\left(r \\rightarrow s\\right)}\n",
        "                = M_{\\mathcal{N}}\\left(\\mathbf{h}_x^{(s)}, \\mathbf{h}_y^{(r)}, \\Theta \\right),\n",
        "\n",
        "        where:\n",
        "\n",
        "        - :math:`\\mathbf{h}_y^{(r)}` are input features on the source cells, called `x_source`,\n",
        "        - :math:`\\mathbf{h}_x^{(s)}` are input features on the target cells, called `x_target`,\n",
        "        - :math:`\\Theta` are optional parameters (weights) of the message passing function.\n",
        "\n",
        "        Optionally, attention can be applied to the message, such that:\n",
        "\n",
        "        .. math::\n",
        "            m_{y \\rightarrow x}^{\\left(r \\rightarrow s\\right)}\n",
        "                \\leftarrow att(\\mathbf{h}_y^{(r)}, \\mathbf{h}_x^{(s)}) . m_{y \\rightarrow x}^{\\left(r \\rightarrow s\\right)}\n",
        "\n",
        "        2. üüß Aggregation: Messages are aggregated across source cells :math:`y` belonging to the\n",
        "        neighborhood :math:`\\mathcal{N}(x)`:\n",
        "\n",
        "        .. math::\n",
        "            m_x^{\\left(r \\rightarrow s\\right)}\n",
        "                = \\text{AGG}_{y \\in \\mathcal{N}(x)} m_{y \\rightarrow x}^{\\left(r\\rightarrow s\\right)},\n",
        "\n",
        "        resulting in the within-neighborhood aggregated message :math:`m_x^{\\left(r \\rightarrow s\\right)}`.\n",
        "\n",
        "        Details in [H23]_ and [PSHM23]_ \"The Steps of Message Passing\".\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_source : Tensor, shape=[..., n_source_cells, in_channels]\n",
        "            Input features on source cells.\n",
        "            Assumes that all source cells have the same rank r.\n",
        "        neighborhood : torch.sparse, shape=[n_target_cells, n_source_cells]\n",
        "            Neighborhood matrix.\n",
        "        x_target : Tensor, shape=[..., n_target_cells, in_channels]\n",
        "            Input features on target cells.\n",
        "            Assumes that all target cells have the same rank s.\n",
        "            Optional. If not provided, x_target is assumed to be x_source,\n",
        "            i.e. source cells send messages to themselves.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        _ : Tensor, shape=[..., n_target_cells, out_channels]\n",
        "            Output features on target cells.\n",
        "            Assumes that all target cells have the same rank s.\n",
        "        \"\"\"\n",
        "        neighborhood = neighborhood.coalesce()\n",
        "        self.target_index_i, self.source_index_j = neighborhood.indices()\n",
        "        neighborhood_values = neighborhood.values()\n",
        "\n",
        "        x_message = self.message(x_source=x_source, x_target=x_target)\n",
        "        x_message = x_message.index_select(-2, self.source_index_j)\n",
        "\n",
        "        if self.att:\n",
        "            attention_values = self.attention(x_source=x_source, x_target=x_target)\n",
        "            neighborhood_values = torch.multiply(neighborhood_values, attention_values)\n",
        "\n",
        "        x_message = neighborhood_values.view(-1, 1) * x_message\n",
        "        return self.aggregate(x_message)\n",
        "\n",
        "\n",
        "class Conv(MessagePassing):\n",
        "    \"\"\"Message passing: steps 1, 2, and 3.\n",
        "\n",
        "    Builds the message passing route given by one neighborhood matrix.\n",
        "    Includes an option for a x-specific update function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_channels : int\n",
        "        Dimension of input features.\n",
        "    out_channels : int\n",
        "        Dimension of output features.\n",
        "    aggr_norm : bool\n",
        "        Whether to normalize the aggregated message by the neighborhood size.\n",
        "    update_func : string\n",
        "        Update method to apply to message.\n",
        "    att : bool\n",
        "        Whether to use attention.\n",
        "        Optional, default: False.\n",
        "    initialization : string\n",
        "        Initialization method.\n",
        "    with_linear_transform: bool\n",
        "        Whether to apply a learnable linear transform.\n",
        "        NB: if false in_channels has to be equal to out_channels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        aggr_norm=False,\n",
        "        update_func=None,\n",
        "        att=False,\n",
        "        initialization=\"xavier_uniform\",\n",
        "        with_linear_transform=True,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            att=att,\n",
        "            initialization=initialization,\n",
        "        )\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.aggr_norm = aggr_norm\n",
        "        self.update_func = update_func\n",
        "\n",
        "        self.weight = (\n",
        "            Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
        "            if with_linear_transform\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        if not with_linear_transform and in_channels != out_channels:\n",
        "            raise ValueError(\n",
        "                \"With `linear_trainsform=False`, in_channels has to be equal to out_channels\"\n",
        "            )\n",
        "        if self.att:\n",
        "            self.att_weight = Parameter(\n",
        "                torch.Tensor(\n",
        "                    2 * self.in_channels,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def update(self, x_message_on_target, x_target=None):\n",
        "        \"\"\"Update embeddings on each cell (step 4).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_message_on_target : torch.Tensor, shape=[n_target_cells, out_channels]\n",
        "            Output features on target cells.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        _ : torch.Tensor, shape=[n_target_cells, out_channels]\n",
        "            Updated output features on target cells.\n",
        "        \"\"\"\n",
        "        if self.update_func == \"sigmoid\":\n",
        "            return torch.sigmoid(x_message_on_target)\n",
        "        if self.update_func == \"relu\":\n",
        "            return torch.nn.functional.relu(x_message_on_target)\n",
        "\n",
        "    def forward(self, x_source, neighborhood, x_target=None):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        This implements message passing:\n",
        "        - from source cells with input features `x_source`,\n",
        "        - via `neighborhood` defining where messages can pass,\n",
        "        - to target cells with input features `x_target`.\n",
        "\n",
        "        In practice, this will update the features on the target cells.\n",
        "\n",
        "        If not provided, x_target is assumed to be x_source,\n",
        "        i.e. source cells send messages to themselves.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_source : Tensor, shape=[..., n_source_cells, in_channels]\n",
        "            Input features on source cells.\n",
        "            Assumes that all source cells have the same rank r.\n",
        "        neighborhood : torch.sparse, shape=[n_target_cells, n_source_cells]\n",
        "            Neighborhood matrix.\n",
        "        x_target : Tensor, shape=[..., n_target_cells, in_channels]\n",
        "            Input features on target cells.\n",
        "            Assumes that all target cells have the same rank s.\n",
        "            Optional. If not provided, x_target is assumed to be x_source,\n",
        "            i.e. source cells send messages to themselves.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        _ : Tensor, shape=[..., n_target_cells, out_channels]\n",
        "            Output features on target cells.\n",
        "            Assumes that all target cells have the same rank s.\n",
        "        \"\"\"\n",
        "        if self.att:\n",
        "            neighborhood = neighborhood.coalesce()\n",
        "            self.target_index_i, self.source_index_j = neighborhood.indices()\n",
        "            attention_values = self.attention(x_source, x_target)\n",
        "            neighborhood = torch.sparse_coo_tensor(\n",
        "                indices=neighborhood.indices(),\n",
        "                values=attention_values * neighborhood.values(),\n",
        "                size=neighborhood.shape,\n",
        "            )\n",
        "        if self.weight is not None:\n",
        "            x_message = torch.mm(x_source, self.weight)\n",
        "        else:\n",
        "            x_message = x_source\n",
        "        x_message_on_target = torch.mm(neighborhood, x_message)\n",
        "\n",
        "        if self.aggr_norm:\n",
        "            neighborhood_size = torch.sum(neighborhood.to_dense(), dim=1)\n",
        "            x_message_on_target = torch.einsum(\n",
        "                \"i,ij->ij\", 1 / neighborhood_size, x_message_on_target\n",
        "            )\n",
        "\n",
        "        if self.update_func is None:\n",
        "            return x_message_on_target\n",
        "\n",
        "        return self.update(x_message_on_target, x_target)\n",
        "\n",
        "\n",
        "class HNHNLayer(torch.nn.Module):\n",
        "    \"\"\"Layer of a Hypergraph Networks with Hyperedge Neurons (HNHN).\n",
        "\n",
        "    Implementation of a simplified version of the HNHN layer proposed in [DSB20]_.\n",
        "\n",
        "    This layer is composed of two convolutional layers:\n",
        "    1. A convolutional layer sending messages from edges to nodes.\n",
        "    2. A convolutional layer sending messages from nodes to edges.\n",
        "    The incidence matrices can be normalized usign the node and edge cardinality.\n",
        "    Two hyperparameters alpha and beta, control the normalization strenght.\n",
        "    The convolutional layers support the training of a bias term.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This is the architecture proposed for node classification.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [DSB20] Dong, Sawin, Bengio.\n",
        "        HNHN: Hypergraph networks with hyperedge neurons.\n",
        "        Graph Representation Learning and Beyond Workshop at ICML 2020\n",
        "        https://grlplus.github.io/papers/40.pdf\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    channels_node : int\n",
        "        Dimension of node features.\n",
        "    channels_edge : int\n",
        "        Dimension of edge features.\n",
        "    incidence_1 : torch.sparse\n",
        "        Incidence matrix mapping edges to nodes (B_1).\n",
        "        shape=[n_nodes, n_edges]\n",
        "    use_bias : bool\n",
        "        Flag controlling whether to use a bias term in the convolution.\n",
        "    use_normalized_incidence : bool\n",
        "        Flag controlling whether to normalize the incidence matrices.\n",
        "    alpha : float\n",
        "        Scalar controlling the importance of edge cardinality.\n",
        "    beta : float\n",
        "        Scalar controlling the importance of node cardinality.\n",
        "    bias_gain : float\n",
        "        Gain for the bias initialization.\n",
        "    bias_init : string [\"xavier_uniform\"|\"xavier_normal\"]\n",
        "        Controls the bias initialization method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels_node,\n",
        "        channels_edge,\n",
        "        incidence_1,\n",
        "        use_bias=True,\n",
        "        use_normalized_incidence=True,\n",
        "        alpha=-1.5,\n",
        "        beta=-0.5,\n",
        "        bias_gain=1.414,\n",
        "        bias_init=\"xavier_uniform\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_bias = use_bias\n",
        "        self.bias_init = bias_init\n",
        "        self.bias_gain = bias_gain\n",
        "        self.use_normalized_incidence = use_normalized_incidence\n",
        "        self.incidence_1 = incidence_1\n",
        "        self.incidence_1_transpose = incidence_1.transpose(1, 0)\n",
        "        self.channels_edge = channels_edge\n",
        "        self.channels_node = channels_node\n",
        "        self.conv_1_to_0 = Conv(\n",
        "            in_channels=channels_edge,\n",
        "            out_channels=channels_node,\n",
        "            aggr_norm=False,\n",
        "            update_func=None,\n",
        "        )\n",
        "        self.conv_0_to_1 = Conv(\n",
        "            in_channels=channels_node,\n",
        "            out_channels=channels_edge,\n",
        "            aggr_norm=False,\n",
        "            update_func=None,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bias_1_to_0 = Parameter(torch.Tensor(1, channels_node))\n",
        "            self.bias_0_to_1 = Parameter(torch.Tensor(1, channels_edge))\n",
        "            self.init_biases()\n",
        "        if self.use_normalized_incidence:\n",
        "            self.alpha = alpha\n",
        "            self.beta = beta\n",
        "            self.n_nodes, self.n_edges = self.incidence_1.shape\n",
        "            self.compute_normalization_matrices()\n",
        "            self.normalize_incidence_matrices()\n",
        "\n",
        "    def compute_normalization_matrices(self):\n",
        "        \"\"\"Compute the normalization matrices for the incidence matrices.\"\"\"\n",
        "        B1 = self.incidence_1.to_dense()\n",
        "        edge_cardinality = (B1.sum(0)) ** self.alpha\n",
        "        node_cardinality = (B1.sum(1)) ** self.beta\n",
        "\n",
        "        # Compute D0_left_alpha_inverse\n",
        "        self.D0_left_alpha_inverse = torch.zeros(self.n_nodes, self.n_nodes)\n",
        "        for i_node in range(self.n_nodes):\n",
        "            self.D0_left_alpha_inverse[i_node, i_node] = 1 / (\n",
        "                edge_cardinality[B1[i_node, :].bool()].sum()\n",
        "            )\n",
        "\n",
        "        # Compute D1_left_beta_inverse\n",
        "        self.D1_left_beta_inverse = torch.zeros(self.n_edges, self.n_edges)\n",
        "        for i_edge in range(self.n_edges):\n",
        "            self.D1_left_beta_inverse[i_edge, i_edge] = 1 / (\n",
        "                node_cardinality[B1[:, i_edge].bool()].sum()\n",
        "            )\n",
        "\n",
        "        # Compute D1_right_alpha\n",
        "        self.D1_right_alpha = torch.diag(edge_cardinality)\n",
        "\n",
        "        # Compute D0_right_beta\n",
        "        self.D0_right_beta = torch.diag(node_cardinality)\n",
        "        return\n",
        "\n",
        "    def normalize_incidence_matrices(self):\n",
        "        \"\"\"Normalize the incidence matrices.\"\"\"\n",
        "        self.incidence_1 = (\n",
        "            self.D0_left_alpha_inverse\n",
        "            @ self.incidence_1.to_dense()\n",
        "            @ self.D1_right_alpha\n",
        "        ).to_sparse()\n",
        "        self.incidence_1_transpose = (\n",
        "            self.D1_left_beta_inverse\n",
        "            @ self.incidence_1_transpose.to_dense()\n",
        "            @ self.D0_right_beta\n",
        "        ).to_sparse()\n",
        "        return\n",
        "\n",
        "    def init_biases(self):\n",
        "        \"\"\"Initialize the bias.\"\"\"\n",
        "        for bias in [self.bias_0_to_1, self.bias_1_to_0]:\n",
        "            if self.bias_init == \"xavier_uniform\":\n",
        "                torch.nn.init.xavier_uniform_(bias, gain=self.bias_gain)\n",
        "            elif self.bias_init == \"xavier_normal\":\n",
        "                torch.nn.init.xavier_normal_(bias, gain=self.bias_gain)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reset learnable parameters.\"\"\"\n",
        "        self.conv_1_to_0.reset_parameters()\n",
        "        self.conv_0_to_1.reset_parameters()\n",
        "        if self.use_bias:\n",
        "            self.init_biases()\n",
        "\n",
        "    def forward(self, x_0, x_1):\n",
        "        r\"\"\"Forward computation.\n",
        "\n",
        "        The forward pass was initially proposed in [DSB20]_.\n",
        "        Its equations are given in [TNN23]_ and graphically illustrated in [PSHM23]_.\n",
        "\n",
        "        The equations of one layer of this neural network are given by:\n",
        "        .. math::\n",
        "        \\begin{align*}\n",
        "        &üü• $\\quad m_{y \\rightarrow x}^{(0 \\rightarrow 1)} = \\sigma((B_1^T \\cdot W^{(0)})_{xy} \\cdot h_y^{t,(0)} \\cdot \\Theta^{t,(0)} + b^{t,(0)})$\n",
        "\n",
        "        &üü• $\\quad m_{y \\rightarrow x}^{(1 \\rightarrow 0)}  = \\sigma((B_1 \\cdot W^{(1)})_{xy} \\cdot h_y^{t,(1)} \\cdot \\Theta^{t,(1)} + b^{t,(1)})$\n",
        "\n",
        "        &üüß $\\quad m_x^{(0 \\rightarrow 1)}  = \\sum_{y \\in \\mathcal{B}(x)} m_{y \\rightarrow x}^{(0 \\rightarrow 1)}$\n",
        "\n",
        "        &üüß $\\quad m_x^{(1 \\rightarrow 0)}  = \\sum_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(1 \\rightarrow 0)}$\n",
        "\n",
        "        &üü© $\\quad m_x^{(0)}  = m_x^{(1 \\rightarrow 0)}$\n",
        "\n",
        "        &üü© $\\quad m_x^{(1)}  = m_x^{(0 \\rightarrow 1)}$\n",
        "\n",
        "        &üü¶ $\\quad h_x^{t+1,(0)}  = m_x^{(0)}$\n",
        "\n",
        "        &üü¶ $\\quad h_x^{t+1,(1)} = m_x^{(1)}$\n",
        "        \\end{align*}\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. [DSB20] Dong, Sawin, Bengio.\n",
        "            HNHN: Hypergraph networks with hyperedge neurons.\n",
        "            Graph Representation Learning and Beyond Workshop at ICML 2020\n",
        "            https://grlplus.github.io/papers/40.pdf\n",
        "        .. [TNN23] Equations of Topological Neural Networks.\n",
        "            https://github.com/awesome-tnns/awesome-tnns/\n",
        "        .. [PSHM23] Papillon, Sanborn, Hajij, Miolane.\n",
        "            Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.\n",
        "            (2023) https://arxiv.org/abs/2304.10031.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_0 : torch.Tensor, shape=[n_nodes, channels_node]\n",
        "            Input features on the hypernodes\n",
        "        x_1 : torch.Tensor, shape=[n_edges, channels_edge]\n",
        "            Input features on the hyperedges\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_0 : torch.Tensor, shape=[n_nodes, channels_node]\n",
        "            Output features on the hypernodes\n",
        "        x_1 : torch.Tensor, shape=[n_edges, channels_edge]\n",
        "            Output features on the hyperedges\n",
        "        \"\"\"\n",
        "        # Move incidence matrices to device\n",
        "        self.incidence_1 = self.incidence_1.to(x_0.device)\n",
        "        self.incidence_1_transpose = self.incidence_1_transpose.to(x_0.device)\n",
        "        # Compute output hyperedge features\n",
        "        x_1_tp1 = self.conv_0_to_1(x_0, self.incidence_1_transpose)  # nodes to edges\n",
        "        if self.use_bias:\n",
        "            x_1_tp1 += self.bias_0_to_1\n",
        "        # Compute output hypernode features\n",
        "        x_0_tp1 = self.conv_1_to_0(x_1, self.incidence_1)  # edges to nodes\n",
        "        if self.use_bias:\n",
        "            x_0_tp1 += self.bias_1_to_0\n",
        "        return torch.sigmoid(x_0_tp1), torch.sigmoid(x_1_tp1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX-i4nU48gn7"
      },
      "source": [
        "### Preprocessing, preliminary data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we lift our graph dataset into the hypergraph domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGzk6Zoi8gn7"
      },
      "outputs": [],
      "source": [
        "import toponetx.datasets.graph as graph\n",
        "\n",
        "\n",
        "dataset_sim = graph.karate_club(complex_type=\"simplicial\")  # import through toponetx\n",
        "dataset_hyp = dataset_sim.to_hypergraph()  # convert simplicial complex to hypergraph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualize our new dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_hypergraph(hypergraph: hypernetx.classes.hypergraph.Hypergraph) -> None:\n",
        "    \"\"\"Visualize a hypergraph.\n",
        "\n",
        "    Args:\n",
        "        hypergraph (hypernetx.classes.hypergraph.Hypergraph): A hypergraph.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    # TODO: draw the hypergraph using hypernetx\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def filter_hypergraph(\n",
        "    hypergraph: hypernetx.classes.hypergraph.Hypergraph, min_size: int, max_size: int\n",
        ") -> hypernetx.classes.hypergraph.Hypergraph:\n",
        "    \"\"\"Returns a hypergraph by keeping only the hyperedges with at most size nodes.\n",
        "\n",
        "    Args:\n",
        "        hypergraph (hypernetx.classes.hypergraph.Hypergraph): A hypergraph.\n",
        "        min_size (int): The minimum size of the hyperedges to keep.\n",
        "        max_size (int): The maximum size of the hyperedges to keep.\n",
        "\n",
        "    Returns:\n",
        "        hypernetx.classes.hypergraph.Hypergraph: A hypergraph.\n",
        "    \"\"\"\n",
        "    return hypernetx.classes.hypergraph.Hypergraph(\n",
        "        {\n",
        "            hyperedge: list(hypergraph.edges[hyperedge])\n",
        "            for hyperedge in list(hypergraph.edges)\n",
        "            if (len(list(hypergraph.edges[hyperedge])) <= max_size)\n",
        "            and (min_size <= len(list(hypergraph.edges[hyperedge])))\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: visualize the hypergraph dataset_hyp using the function above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_hypergraph = filter_hypergraph(dataset_hyp, min_size=2, max_size=2)\n",
        "# TODO: visualize the hypergraph reduced_hypergraph using the function above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anogWyxU8gn7"
      },
      "source": [
        "#### Define neighborhood structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the hypergraph, we now retrieve the neighborhood structures that we will use in the message passing. In our case, we just need the boundary matrix (or incidence matrix) $B_1$ (or $B_{0,1}$). The shape of the matrix is: $(n_\\text{nodes} \\times n_\\text{edges})$. We will also convert the neighborhood structures to sparse torch tensors that will be feed into our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvOimz6g8gn8",
        "outputId": "b30cb006-ce8f-4f09-adcf-66fe1d97ee9a"
      },
      "outputs": [],
      "source": [
        "incidence_1 = dataset_sim.incidence_matrix(rank=1, signed=False)\n",
        "incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
        "print(f\"The incidence matrix B1 has shape: {incidence_1.shape}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjWud-ZV8gn8"
      },
      "source": [
        "#### Import the features and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our features will be:\n",
        "- node features $X_0$ of shape: $n_{nodes} \\times channels_{node}$\n",
        "- edge features $X_1$ of shape: $n_{edges} \\times channels_{edge}$\n",
        "\n",
        "In our case, we have $channels_{node}$ = $channels_{edge}$ = 2. The features are the eigenvalues of the hodge laplacian. Next, we will retrieve node features, edge features, and node labels $y$ that are one hot encoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjLMOMde8gn8",
        "outputId": "6c6eaf71-f70c-4a4e-b323-1f8061a7309a"
      },
      "outputs": [],
      "source": [
        "# Node features\n",
        "x_0 = []\n",
        "for _, v in dataset_sim.get_simplex_attributes(\"node_feat\").items():\n",
        "    x_0.append(v)\n",
        "x_0 = torch.tensor(np.stack(x_0)).to(device)\n",
        "n_nodes, channels_node = x_0.shape\n",
        "print(f\"There are {n_nodes} nodes with features of dimension {channels_node}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5nvUxwN8gn8",
        "outputId": "84f7d1ca-2287-4f69-9bdf-9008201df15f"
      },
      "outputs": [],
      "source": [
        "# Edge features\n",
        "x_1 = []\n",
        "for k, v in dataset_sim.get_simplex_attributes(\"edge_feat\").items():\n",
        "    x_1.append(v)\n",
        "x_1 = torch.tensor(np.stack(x_1)).to(device)\n",
        "n_edges, channels_edge = x_1.shape\n",
        "print(f\"There are {n_edges} edges with features of dimension {channels_edge}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypYQDBGU8gn9",
        "outputId": "c561e2e0-ecef-4aaf-e5bd-e7c106ff0240"
      },
      "outputs": [],
      "source": [
        "# Node labels\n",
        "y = np.array(\n",
        "    [\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        0,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        1,\n",
        "        0,\n",
        "        0,\n",
        "        1,\n",
        "        1,\n",
        "        0,\n",
        "        1,\n",
        "        0,\n",
        "        1,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "        0,\n",
        "    ]\n",
        ")\n",
        "n_classes = len(np.unique(y))\n",
        "y_1h = np.eye(n_classes)[y].astype(int)  # 1-hot representation\n",
        "print(f\"There are {y_1h.shape[0]} labels, one for each node.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqdZhfuz8gn9"
      },
      "source": [
        "We will now split the dataset into a train (80%) and test set (20%). To deal with label imbalance, we will stratify our split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4O1LByb8gn9",
        "outputId": "d77b1862-e691-477a-d93f-6ac6efe7ac19"
      },
      "outputs": [],
      "source": [
        "test_size = 0.2  # 20%\n",
        "y_train, y_test, ind_train, ind_test = train_test_split(\n",
        "    y_1h, np.arange(n_nodes), test_size=test_size, random_state=seed, stratify=y\n",
        ")\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "y = torch.tensor(y, dtype=torch.int32).to(device)\n",
        "print(\n",
        "    f\"Fraction of class-1 samples in the training set: {round(100 * torch.sum(y_train[:,0]).item() / y_train.shape[0], 3)}%\"\n",
        ")\n",
        "print(\n",
        "    f\"Fraction of class-1 samples in the test set: {round(100 * torch.sum(y_test[:,0]).item() / y_test.shape[0], 3)}%\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woE5SsPN8gn9"
      },
      "source": [
        "### Create the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the HNHNLayer class, we create a neural network for node classification. We will call it HNHNNetwork."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2WN_ucY8gn9"
      },
      "outputs": [],
      "source": [
        "class HNHNNetwork(torch.nn.Module):\n",
        "    \"\"\"Hypergraph Networks with Hyperedge Neurons. Implementation for multiclass node classification.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels_node: int,\n",
        "        channels_edge: int,\n",
        "        incidence_1: torch.sparse,\n",
        "        n_classes: int,\n",
        "        n_layers: int = 2,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the model.\n",
        "\n",
        "        Args:\n",
        "            channels_node (int): Dimension of node features.\n",
        "            channels_edge (int): Dimension of edge features.\n",
        "            incidence_1 (torch.sparse): Incidence matrix mapping edges to nodes (B_1).\n",
        "                shape=[n_nodes, n_edges]\n",
        "            n_classes (int): Number of classes\n",
        "            n_layers (int, optional): Number of HNHN message passing layers. Defaults to 2.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList(\n",
        "            [\n",
        "                HNHNLayer(\n",
        "                    channels_node=channels_node,\n",
        "                    channels_edge=channels_edge,\n",
        "                    incidence_1=incidence_1,\n",
        "                )\n",
        "                for _ in range(n_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.linear = torch.nn.Linear(\n",
        "            channels_node, n_classes\n",
        "        )  # final prediction layer\n",
        "\n",
        "    def forward(\n",
        "        self, x_0: torch.Tensor, x_1: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward computation.\n",
        "\n",
        "        Args:\n",
        "            x_0 (torch.Tensor): shape = [n_nodes, channels_node]\n",
        "                    Hypernode features.\n",
        "            x_1 (torch.Tensor): shape = [n_nodes, channels_edge]\n",
        "                Hyperedge features.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: The predicted node logits with shape = [n_nodes, n_classes],\n",
        "                and The predicted node class with shape = [n_nodes].\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x_0, x_1 = layer(x_0, x_1)\n",
        "        logits = self.linear(x_0)\n",
        "        classes = torch.softmax(logits, -1).argmax(-1)\n",
        "        return logits, classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6wGrjww8gn9"
      },
      "source": [
        "### Train the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We initialize the HNHNNetwork model with our neighborhood structures and specify the same optimizer and loss as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYrqpeuA8gn9"
      },
      "outputs": [],
      "source": [
        "n_layers = 2\n",
        "learning_rate = 0.5 * 1e-2\n",
        "\n",
        "model = HNHNNetwork(\n",
        "    channels_node=channels_node,\n",
        "    channels_edge=channels_edge,\n",
        "    incidence_1=incidence_1,\n",
        "    n_classes=n_classes,\n",
        "    n_layers=n_layers,\n",
        ").to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the number of parameters of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Number of parameters HNHNNetwork: {get_nb_parameters(model)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Way less parameters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh5B9trK8gn-"
      },
      "source": [
        "Next, we train the model for 2000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VW1lgzOK8gn-",
        "outputId": "5c7a787e-7cba-44c4-f1f4-e429b69d31e9"
      },
      "outputs": [],
      "source": [
        "def train_HNHN_one_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    data: Tuple[\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "    ],\n",
        ") -> Tuple[float, float, float, float, torch.Tensor]:\n",
        "    \"\"\"Train the model for 1 epoch.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        data (Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]): The graph data (x_0, x_1, y_train, y_test, ind_train, ind_test).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float, float, float, torch.Tensor]: The losses on train and test, the accuracy on train and test, and the embedding h after one epoch/step.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # clear gradients.\n",
        "    x_0, x_1, y_train, y_test, ind_train, ind_test = data\n",
        "    logits, classes = model(x_0, x_1)  # perform a single forward pass.\n",
        "    loss_train = criterion(\n",
        "        logits[ind_train], y_train\n",
        "    )  # compute the loss on the training nodes.\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # no gradients for test pass.\n",
        "        loss_test = criterion(logits[ind_test], y_test)\n",
        "        acc_train = torch.sum(classes[ind_train] == y_train.argmax(dim=1)).item() / len(\n",
        "            ind_train\n",
        "        )\n",
        "        acc_test = torch.sum(classes[ind_test] == y_test.argmax(dim=1)).item() / len(\n",
        "            ind_test\n",
        "        )\n",
        "    model.train()\n",
        "    loss_train.backward()  # derive gradients.\n",
        "    optimizer.step()  # update parameters based on gradients.\n",
        "    return loss_train.item(), loss_test.item(), acc_train, acc_test, x_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_epochs = 1000  # TODO: change the number of epochs?\n",
        "data = [\n",
        "    x_0,\n",
        "    x_1,\n",
        "    y_train,\n",
        "    y_test,\n",
        "    ind_train,\n",
        "    ind_test,\n",
        "]\n",
        "losses_train, losses_test, accuracy_train, accuracy_test, embeddings = train(\n",
        "    model, data, nb_epochs, train_HNHN_one_epoch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4guSxB28gn-"
      },
      "source": [
        "Finally, we plot the training results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot the learning curves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNep3G0KDEqW"
      },
      "source": [
        "## Healthcare application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's apply our models to QM9!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define first some utility functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mols_to_nx(mols: List[Chem.Mol]) -> List[nx.Graph]:\n",
        "    \"\"\"Converts a list of molecules to a list of networkx graphs.\n",
        "\n",
        "    Args:\n",
        "        mols (List[Chem.Mol]): list of molecules\n",
        "\n",
        "    Returns:\n",
        "        List[nx.Graph]: list of networkx graphs\n",
        "    \"\"\"\n",
        "    nx_graphs = []\n",
        "    for mol in mols:\n",
        "        G = nx.Graph()\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            G.add_node(atom.GetIdx(), label=atom.GetSymbol())\n",
        "            # Other potential features:\n",
        "            # atomic_num=atom.GetAtomicNum()\n",
        "            # formal_charge=atom.GetFormalCharge()\n",
        "            # chiral_tag=atom.GetChiralTag()\n",
        "            # hybridization=atom.GetHybridization()\n",
        "            # num_explicit_hs=atom.GetNumExplicitHs()\n",
        "            # is_aromatic=atom.GetIsAromatic()\n",
        "\n",
        "        for bond in mol.GetBonds():\n",
        "            G.add_edge(\n",
        "                bond.GetBeginAtomIdx(),\n",
        "                bond.GetEndAtomIdx(),\n",
        "                label=int(bond.GetBondTypeAsDouble()),\n",
        "            )\n",
        "            # Other potential feature:\n",
        "            # bond_type=bond.GetBondType()\n",
        "\n",
        "        nx_graphs.append(G)\n",
        "    return nx_graphs\n",
        "\n",
        "\n",
        "def pad_adjs(ori_adj: np.ndarray, node_number: int) -> np.ndarray:\n",
        "    \"\"\"Create padded adjacency matrices\n",
        "\n",
        "    Args:\n",
        "        ori_adj (np.ndarray): original adjacency matrix\n",
        "        node_number (int): number of desired nodes\n",
        "\n",
        "    Raises:\n",
        "        ValueError: if the original adjacency matrix is larger than the desired number of nodes (we can't pad)\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Padded adjacency matrix\n",
        "    \"\"\"\n",
        "    if not (ori_adj.size):  # empty\n",
        "        return np.zeros((node_number, node_number), dtype=np.float32)\n",
        "    a = ori_adj\n",
        "    ori_len = a.shape[-1]\n",
        "    if ori_len == node_number:  # same shape\n",
        "        return a\n",
        "    if ori_len > node_number:\n",
        "        raise ValueError(\n",
        "            f\"Original number of nodes {ori_len} is greater (>) that the desired number of nodes after padding {node_number}\"\n",
        "        )\n",
        "    # Pad\n",
        "    a = np.concatenate([a, np.zeros([ori_len, node_number - ori_len])], axis=-1)\n",
        "    a = np.concatenate([a, np.zeros([node_number - ori_len, node_number])], axis=0)\n",
        "    return a\n",
        "\n",
        "\n",
        "def graphs_to_tensor(graph_list: List[nx.Graph], max_node_num: int) -> torch.Tensor:\n",
        "    \"\"\"Convert a list of graphs to a tensor\n",
        "\n",
        "    Args:\n",
        "        graph_list (List[nx.Graph]): List of graphs to convert to adjacency matrices tensors\n",
        "        max_node_num (int): max number of nodes in all the graphs\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of adjacency matrices\n",
        "    \"\"\"\n",
        "    adjs_list = []\n",
        "    max_node_num = max_node_num  # memory issue\n",
        "\n",
        "    for g in graph_list:\n",
        "        assert isinstance(g, nx.Graph)\n",
        "        node_list = []\n",
        "        for v, feature in g.nodes.data(\"feature\"):\n",
        "            node_list.append(v)\n",
        "\n",
        "        # convert to adj matrix\n",
        "        adj = nx.to_numpy_array(g, nodelist=node_list)\n",
        "        padded_adj = pad_adjs(adj, node_number=max_node_num)  # pad to max node number\n",
        "        adjs_list.append(padded_adj)\n",
        "\n",
        "    del graph_list\n",
        "\n",
        "    adjs_np = np.asarray(adjs_list)  # concatenate the arrays\n",
        "    del adjs_list\n",
        "\n",
        "    adjs_tensor = torch.tensor(adjs_np, dtype=torch.float32)  # convert to tensor\n",
        "    del adjs_np\n",
        "\n",
        "    return adjs_tensor\n",
        "\n",
        "\n",
        "def plot_molecules(mols: List[Chem.Mol], max_num: int = 16, shift: int = 100) -> None:\n",
        "    \"\"\"Plot multiple molecules (max_num) at the same time.\n",
        "\n",
        "    Args:\n",
        "        mols (List[Chem.Mol]): List of molecules to plot.\n",
        "        max_num (int, optional): number of molecules to plot in a square image. Defaults to 16.\n",
        "        shift (int, optional): shift to plot starting at index shift. Defaults to 100.\n",
        "    \"\"\"\n",
        "    img_c = int(math.ceil(np.sqrt(max_num)))\n",
        "    figure = plt.figure()\n",
        "\n",
        "    for idx in range(max_num):\n",
        "        mol = mols[idx + shift]\n",
        "\n",
        "        assert isinstance(\n",
        "            mol, Chem.Mol\n",
        "        ), \"elements should be molecules\"  # check if we have a molecule\n",
        "\n",
        "        ax = plt.subplot(img_c, img_c, idx + 1)\n",
        "        mol_img = Draw.MolToImage(mol, size=(300, 300))\n",
        "        ax.imshow(mol_img)\n",
        "        title_str = f\"{Chem.MolToSmiles(mol)}\"\n",
        "        ax.title.set_text(title_str)\n",
        "        ax.set_axis_off()\n",
        "    figure.suptitle(\"Plot of molecules\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def convert_adjacency_to_edge_index(A: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Converts an adjacency matrix to edge indices for PyTorch Geometric.\n",
        "    If data.edge_index.size()[1] == 0, then the graph is empty or only made of nodes. Let's add self loops.\n",
        "\n",
        "    Args:\n",
        "        A (torch.Tensor): Adjacency matrix\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Edge indices\n",
        "    \"\"\"\n",
        "    A_ = A.to_dense()\n",
        "    edge_index = torch.nonzero(A_, as_tuple=False).t().contiguous().int()\n",
        "    if not (edge_index.size()[1]):\n",
        "        edge_index = torch.tensor([[0], [0]]).int()\n",
        "    return edge_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykWrUOK_DGc_"
      },
      "outputs": [],
      "source": [
        "def load_qm9(\n",
        "    folder: str = \"./\", nb_mol: int = 100, features: List[str] = [\"mu\", \"alpha\"]\n",
        ") -> Tuple[\n",
        "    List[torch_geometric.data.data.Data],\n",
        "    List[torch_geometric.data.data.Data],\n",
        "    List[Chem.Mol],\n",
        "    List[Chem.Mol],\n",
        "    List[torch.Tensor],\n",
        "    List[torch.Tensor],\n",
        "]:\n",
        "    \"\"\"Load the QM9 dataset.\n",
        "\n",
        "    Args:\n",
        "        folder (str, optional): path to the folder where the qm9.csv file is located. Defaults to \"./\".\n",
        "        nb_mol (int, optional): number of molecules to load. Defaults to 1000.\n",
        "        features (List[str], optional): features to load. Defaults to [\"mu\", \"alpha\"].\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[torch_geometric.data.data.Data], List[torch_geometric.data.data.Data], List[Chem.Mol], List[Chem.Mol], List[torch.Tensor], List[torch.Tensor]]: List of PyTorch Geometric Data objects, one for each molecules, for train and test, the lists of molecules finally the results of most of them.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(os.path.join(folder, \"qm9.csv\"))\n",
        "    data = data[[\"SMILES1\"] + features][:nb_mol]\n",
        "\n",
        "    max_node_num = 9  # max number of nodes in the QM9 dataset\n",
        "\n",
        "    with open(os.path.join(folder, f\"valid_idx_qm9.json\")) as f:\n",
        "        test_idx = json.load(f)\n",
        "\n",
        "    test_idx = test_idx[\"valid_idxs\"]\n",
        "    test_idx = set([int(i) for i in test_idx if int(i) < nb_mol])\n",
        "    train_idx = set(\n",
        "        [i for i in range(len(data)) if i not in test_idx and (int(i) < nb_mol)]\n",
        "    )\n",
        "\n",
        "    y_train = torch.tensor(\n",
        "        data[features].values[np.array(list(train_idx))], dtype=torch.float32\n",
        "    )\n",
        "    y_test = torch.tensor(\n",
        "        data[features].values[np.array(list(test_idx))], dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    train_mols = [\n",
        "        Chem.MolFromSmiles(smiles)\n",
        "        for i, smiles in enumerate(data[\"SMILES1\"])\n",
        "        if i in train_idx\n",
        "    ]\n",
        "    train_graphs = mols_to_nx(train_mols)\n",
        "    test_mols = [\n",
        "        Chem.MolFromSmiles(smiles)\n",
        "        for i, smiles in enumerate(data[\"SMILES1\"])\n",
        "        if i in test_idx\n",
        "    ]\n",
        "    test_graphs = mols_to_nx(test_mols)\n",
        "    return (\n",
        "        [\n",
        "            torch_geometric.data.data.Data(\n",
        "                x=torch.diag(\n",
        "                    torch.tensor(\n",
        "                        [\n",
        "                            1.0 if i in set(list(g.nodes)) else 0.0\n",
        "                            for i in range(max_node_num)\n",
        "                        ],\n",
        "                        dtype=torch.float32,\n",
        "                    )\n",
        "                ),\n",
        "                edge_index=convert_adjacency_to_edge_index(\n",
        "                    graphs_to_tensor([g], max_node_num=max_node_num)[0]\n",
        "                ),\n",
        "            )\n",
        "            for i, g in enumerate(train_graphs)\n",
        "        ],\n",
        "        [\n",
        "            torch_geometric.data.data.Data(\n",
        "                x=torch.diag(\n",
        "                    torch.tensor(\n",
        "                        [\n",
        "                            1.0 if i in set(list(g.nodes)) else 0.0\n",
        "                            for i in range(max_node_num)\n",
        "                        ],\n",
        "                        dtype=torch.float32,\n",
        "                    )\n",
        "                ),\n",
        "                edge_index=convert_adjacency_to_edge_index(\n",
        "                    graphs_to_tensor([g], max_node_num=max_node_num)[0]\n",
        "                ),\n",
        "            )\n",
        "            for i, g in enumerate(test_graphs)\n",
        "        ],\n",
        "        train_mols,\n",
        "        test_mols,\n",
        "        y_train,\n",
        "        y_test,\n",
        "    )\n",
        "\n",
        "\n",
        "def print_top_qm9_and_features(folder: str = \"./\") -> None:\n",
        "    \"\"\"Print the first lines of QM9 and the list of all features.\n",
        "\n",
        "    Args:\n",
        "        folder (str, optional): path to the folder that contains qm9.csv. Defaults to \"./\".\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(os.path.join(folder, \"qm9.csv\"))\n",
        "    print(\"Features QM9 dataset:\")\n",
        "    print(list(data.columns))\n",
        "    print(\"First 5 rows:\")\n",
        "    print(data.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_top_qm9_and_features(folder=\"./\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the QM9 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder = \"./\"\n",
        "nb_mol = 100  # TODO: try to increase the number of molecules\n",
        "features = [\"mu\", \"alpha\"]  # TODO: try to add more features or some other features\n",
        "train_data, test_data, train_mols, test_mols, y_train, y_test = load_qm9(\n",
        "    folder=folder, nb_mol=nb_mol, features=features\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot some molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_molecules(train_mols, max_num=16, shift=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_node_number = 9\n",
        "model = GCN(\n",
        "    num_features=max_node_number,\n",
        "    hidden_dim=4,\n",
        "    embedding_dim=2,\n",
        "    num_classes=len(features),\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Number of parameters GCN: {get_nb_parameters(model)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.01  # TODO: change the learning rate?\n",
        "\n",
        "criterion = torch.nn.MSELoss()  # mean squared error (MSE) loss for regression.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Small modification to our GNN, to make a prediction for the entire graph, we will average the predictions made on each nodes!\n",
        "\n",
        "This is done below by adding the line `out = out.avg(dim=0)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch_reg(\n",
        "    model: torch.nn.Module,\n",
        "    data_train: torch_geometric.data.data.Data,\n",
        "    data_test: torch_geometric.data.data.Data,\n",
        "    y_train: torch.Tensor,\n",
        "    y_test: torch.Tensor,\n",
        ") -> Tuple[float, float, float, float, torch.Tensor]:\n",
        "    \"\"\"Train the model for 1 epoch.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        data_train (torch_geometric.data.data.Data): The train graph data.\n",
        "        data_test (torch_geometric.data.data.Data): The test graph data.\n",
        "        y_train (torch.Tensor): The train labels.\n",
        "        y_test (torch.Tensor): The test labels.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float, float, float, torch.Tensor]: The losses on train and test, the MSE on train and test, and the embedding h after one epoch/step.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # clear gradients.\n",
        "    avg_train_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "    avg_train_mse = torch.tensor(0.0, dtype=torch.float32)\n",
        "    for i, data in enumerate(data_train):\n",
        "        out, h = model(data.x, data.edge_index)  # perform a single forward pass.\n",
        "        out = out.mean(\n",
        "            dim=0\n",
        "        )  # average all the node predictions for the graph prediction!\n",
        "        loss_train = criterion(out, y_train[i])  # compute the loss\n",
        "        avg_train_loss += loss_train\n",
        "        avg_train_mse += mean_squared_error(y_train[i], out.detach().numpy())\n",
        "    avg_train_loss /= len(data_train)\n",
        "    avg_train_mse /= len(data_train)\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # no gradients for test pass.\n",
        "        avg_test_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "        avg_test_mse = torch.tensor(0.0, dtype=torch.float32)\n",
        "        for i, data in enumerate(data_test):\n",
        "            out, h = model(data.x, data.edge_index)\n",
        "            out = out.mean(\n",
        "                dim=0\n",
        "            )  # average all the node predictions for the graph prediction!\n",
        "            loss_test = criterion(out, y_test[i])\n",
        "            avg_test_loss += loss_test\n",
        "            avg_test_mse += mean_squared_error(y_test[i], out.detach().numpy())\n",
        "        avg_test_loss /= len(data_test)\n",
        "        avg_test_mse /= len(data_test)\n",
        "    model.train()\n",
        "    avg_train_loss.backward()  # derive gradients.\n",
        "    optimizer.step()  # update parameters based on gradients.\n",
        "    return avg_train_loss.item(), avg_test_loss.item(), avg_train_mse, avg_test_mse, h\n",
        "\n",
        "\n",
        "def train_reg(\n",
        "    model: torch.nn.Module,\n",
        "    data_train: torch_geometric.data.data.Data,\n",
        "    data_test: torch_geometric.data.data.Data,\n",
        "    y_train: torch.Tensor,\n",
        "    y_test: torch.Tensor,\n",
        "    nb_epochs: int,\n",
        "    train_func: Callable[\n",
        "        [\n",
        "            torch.nn.Module,\n",
        "            torch_geometric.data.data.Data,\n",
        "            torch_geometric.data.data.Data,\n",
        "        ],\n",
        "        Tuple[float, float, float, float, torch.Tensor],\n",
        "    ],\n",
        ") -> Tuple[List[float], List[float], List[float], List[float], List[torch.Tensor]]:\n",
        "    \"\"\"Train the model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        data_train (torch_geometric.data.data.Data): The train graph data.\n",
        "        data_test (torch_geometric.data.data.Data): The test graph data.\n",
        "        y_train (torch.Tensor): The train labels.\n",
        "        y_test (torch.Tensor): The test labels.\n",
        "        nb_epochs (int): The number of epochs.\n",
        "        train_func (Callable[[torch.nn.Module, Union[torch_geometric.data.data.Data, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]], Tuple[float, float, float, float torch.Tensor]]): function that train our model for one epoch\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[float], List[float], List[float], List[float], List[torch.Tensor]]: The losses on train and test, the MSE on train and test, and the embeddings at each epochs.\n",
        "    \"\"\"\n",
        "    top = perf_counter()  # calculate total training time\n",
        "    model.train()\n",
        "    losses_train = []\n",
        "    losses_test = []\n",
        "    mse_train = []\n",
        "    mse_test = []\n",
        "    embeddings = []\n",
        "    for epoch in tqdm(range(1, nb_epochs + 1)):\n",
        "        loss_train, loss_test, m_train, m_test, h = train_func(\n",
        "            model, data_train, data_test, y_train, y_test\n",
        "        )\n",
        "        losses_train.append(loss_train)\n",
        "        losses_test.append(loss_test)\n",
        "        mse_train.append(m_train)\n",
        "        mse_test.append(m_test)\n",
        "        embeddings.append(h)\n",
        "    print(f\"Training finished! Total time: {round(perf_counter() - top, 3)}s.\")\n",
        "    return losses_train, losses_test, mse_train, mse_test, embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_epochs = 300  # TODO: change the number of epochs?\n",
        "losses_train, losses_test, mse_train, mse_test, embeddings = train_reg(\n",
        "    model, train_data, test_data, y_train, y_test, nb_epochs, train_one_epoch_reg\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_learning_curves_reg(\n",
        "    losses_train: List[float],\n",
        "    losses_test: List[float],\n",
        "    mse_train: List[float],\n",
        "    mse_test: List[float],\n",
        ") -> None:\n",
        "    \"\"\"Plot the learning curves.\n",
        "\n",
        "    Args:\n",
        "        losses_train (List[float]): The training losses.\n",
        "        losses_test (List[float]): The test losses.\n",
        "        mse_train (List[float]): The training mean squared errors.\n",
        "        mse_test (List[float]): The test mean squared errors.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(losses_train, label=\"train\")\n",
        "    plt.plot(losses_test, label=\"test\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(100 * np.array(mse_train), label=\"train\")\n",
        "    plt.plot(100 * np.array(mse_test), label=\"test\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Mean squared errors\")\n",
        "    plt.legend()\n",
        "    plt.suptitle(\n",
        "        \"Learning curves. Final train mse: {:.2f}. Final test mse: {:.2f}\".format(\n",
        "            mse_train[-1], mse_test[-1]\n",
        "        )\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_learning_curves_reg(losses_train, losses_test, mse_train, mse_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Whay do you think? How to improve?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "my_conda_env_with_py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
